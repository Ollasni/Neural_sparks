"""
RAG Retriever Module for BI-GPT Agent
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å—Ö–µ–º–µ –ë–î –∏ –±–∏–∑–Ω–µ—Å-—Å–ª–æ–≤–∞—Ä—é
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü, –∫–æ–ª–æ–Ω–æ–∫, —Å–≤—è–∑–µ–π –∏ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–æ–≤
"""

import json
import re
import logging
import math
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict
import sqlite3

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    content: str
    relevance_score: float
    result_type: str  # 'table', 'column', 'business_term', 'relationship'
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score –≤ –¥–∏–∞–ø–∞–∑–æ–Ω 0-1
        self.relevance_score = max(0, min(1, self.relevance_score))


@dataclass
class BusinessTerm:
    """–ë–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    term: str
    definition: str
    synonyms: List[str] = field(default_factory=list)
    category: str = "general"
    examples: List[str] = field(default_factory=list)
    related_tables: List[str] = field(default_factory=list)
    related_columns: List[str] = field(default_factory=list)


class BusinessDictionary:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –±–∏–∑–Ω–µ—Å-—Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        self.terms: Dict[str, BusinessTerm] = {}
        self._init_default_terms()
    
    def _init_default_terms(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–æ–≤"""
        default_terms = [
            BusinessTerm(
                term="–ø—Ä–∏–±—ã–ª—å",
                definition="revenue - costs",
                synonyms=["–¥–æ—Ö–æ–¥", "–ø—Ä–æ—Ñ–∏—Ç", "–≤—ã–≥–æ–¥–∞", "–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å"],
                category="—Ñ–∏–Ω–∞–Ω—Å—ã",
                examples=["–ø—Ä–∏–±—ã–ª—å –∑–∞ –º–µ—Å—è—Ü", "–æ–±—â–∞—è –ø—Ä–∏–±—ã–ª—å"],
                related_columns=["revenue", "costs", "profit"]
            ),
            BusinessTerm(
                term="–º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å",
                definition="(revenue - costs) / revenue * 100",
                synonyms=["–º–∞—Ä–∂–∞", "—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å", "–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å"],
                category="—Ñ–∏–Ω–∞–Ω—Å—ã",
                examples=["–º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", "—Å—Ä–µ–¥–Ω—è—è –º–∞—Ä–∂–∞"],
                related_columns=["revenue", "costs"]
            ),
            BusinessTerm(
                term="—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫",
                definition="AVG(order_amount)",
                synonyms=["—Å—Ä–µ–¥–Ω–∏–π –∑–∞–∫–∞–∑", "AOV", "average order value"],
                category="–ø—Ä–æ–¥–∞–∂–∏",
                examples=["—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤", "AOV –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º"],
                related_tables=["orders"],
                related_columns=["amount", "order_amount"]
            ),
            BusinessTerm(
                term="–≤—ã—Ä—É—á–∫–∞",
                definition="SUM(revenue)",
                synonyms=["–æ–±–æ—Ä–æ—Ç", "–¥–æ—Ö–æ–¥—ã", "–ø—Ä–æ–¥–∞–∂–∏"],
                category="—Ñ–∏–Ω–∞–Ω—Å—ã",
                examples=["–æ–±—â–∞—è –≤—ã—Ä—É—á–∫–∞", "–≤—ã—Ä—É—á–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥"],
                related_columns=["revenue", "amount", "sales"]
            ),
            BusinessTerm(
                term="–æ—Å—Ç–∞—Ç–∫–∏",
                definition="current_stock",
                synonyms=["—Å–∫–ª–∞–¥", "–∑–∞–ø–∞—Å—ã", "–∏–Ω–≤–µ–Ω—Ç–∞—Ä—å", "stock"],
                category="–ª–æ–≥–∏—Å—Ç–∏–∫–∞",
                examples=["–æ—Å—Ç–∞—Ç–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤", "–æ—Å—Ç–∞—Ç–∫–∏ –Ω–∞ —Å–∫–ª–∞–¥–µ"],
                related_tables=["inventory"],
                related_columns=["current_stock", "stock", "quantity"]
            ),
            BusinessTerm(
                term="–∫–ª–∏–µ–Ω—Ç—ã",
                definition="customers",
                synonyms=["–ø–æ–∫—É–ø–∞—Ç–µ–ª–∏", "–∑–∞–∫–∞–∑—á–∏–∫–∏", "users", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏"],
                category="CRM",
                examples=["–∞–∫—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã", "–Ω–æ–≤—ã–µ –∫–ª–∏–µ–Ω—Ç—ã"],
                related_tables=["customers", "users"]
            ),
            BusinessTerm(
                term="–∑–∞–∫–∞–∑—ã",
                definition="orders",
                synonyms=["–ø–æ–∫—É–ø–∫–∏", "—Å–¥–µ–ª–∫–∏", "—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"],
                category="–ø—Ä–æ–¥–∞–∂–∏",
                examples=["–Ω–æ–≤—ã–µ –∑–∞–∫–∞–∑—ã", "–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–∫–∞–∑—ã"],
                related_tables=["orders", "sales"]
            ),
            BusinessTerm(
                term="—Ç–æ–≤–∞—Ä—ã",
                definition="products",
                synonyms=["–ø—Ä–æ–¥—É–∫—Ç—ã", "items", "–Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä–∞"],
                category="–∫–∞—Ç–∞–ª–æ–≥",
                examples=["–ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã", "–Ω–æ–≤—ã–µ —Ç–æ–≤–∞—Ä—ã"],
                related_tables=["products", "items"]
            )
        ]
        
        for term in default_terms:
            self.add_term(term)
    
    def add_term(self, term: BusinessTerm):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ—Ä–º–∏–Ω –≤ —Å–ª–æ–≤–∞—Ä—å"""
        self.terms[term.term.lower()] = term
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        for synonym in term.synonyms:
            self.terms[synonym.lower()] = term
    
    def find_term(self, query: str) -> Optional[BusinessTerm]:
        """–ò—â–µ—Ç —Ç–µ—Ä–º–∏–Ω –≤ —Å–ª–æ–≤–∞—Ä–µ"""
        query_lower = query.lower().strip()
        return self.terms.get(query_lower)
    
    def search_terms(self, query: str, threshold: float = 0.3) -> List[Tuple[BusinessTerm, float]]:
        """–ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        query_lower = query.lower()
        results = []
        
        for term_key, term in self.terms.items():
            if term in [t[0] for t in results]:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                continue
            
            score = 0.0
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if term_key in query_lower:
                score = 1.0
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            elif any(word in query_lower for word in term_key.split()):
                score = 0.7
            # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏
            elif term_key in term.definition.lower():
                score = 0.5
            # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö
            elif any(term_key in example.lower() for example in term.examples):
                score = 0.4
            
            if score >= threshold:
                results.append((term, score))
        
        return sorted(results, key=lambda x: x[1], reverse=True)


class SimpleVectorizer:
    """–ü—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF –±–µ–∑ sklearn"""
    
    def __init__(self, max_features: int = 1000):
        self.max_features = max_features
        self.vocabulary_: Dict[str, int] = {}
        self.idf_: Dict[str, float] = {}
        self.documents_: List[str] = []
    
    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Å–ª–æ–≤–∞–º
        words = re.findall(r'\b\w+\b', text.lower())
        return words
    
    def fit(self, documents: List[str]):
        """–û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä"""
        self.documents_ = documents
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        word_doc_count = defaultdict(int)
        all_words = set()
        
        for doc in documents:
            words = set(self._tokenize(doc))
            all_words.update(words)
            for word in words:
                word_doc_count[word] += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        sorted_words = sorted(word_doc_count.items(), key=lambda x: x[1], reverse=True)
        vocab_words = [word for word, count in sorted_words[:self.max_features]]
        
        self.vocabulary_ = {word: i for i, word in enumerate(vocab_words)}
        
        # –í—ã—á–∏—Å–ª—è–µ–º IDF
        n_docs = len(documents)
        for word in self.vocabulary_:
            df = word_doc_count[word]
            self.idf_[word] = math.log(n_docs / (1 + df))
    
    def transform(self, documents: List[str]) -> List[List[float]]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä—ã"""
        if not self.vocabulary_:
            raise ValueError("Vectorizer not fitted")
        
        vectors = []
        
        for doc in documents:
            words = self._tokenize(doc)
            word_count = defaultdict(int)
            
            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
            for word in words:
                if word in self.vocabulary_:
                    word_count[word] += 1
            
            # –°–æ–∑–¥–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä
            vector = [0.0] * len(self.vocabulary_)
            
            if words:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                for word, count in word_count.items():
                    if word in self.vocabulary_:
                        tf = count / len(words)
                        idf = self.idf_[word]
                        idx = self.vocabulary_[word]
                        vector[idx] = tf * idf
            
            vectors.append(vector)
        
        return vectors
    
    def fit_transform(self, documents: List[str]) -> List[List[float]]:
        """–û–±—É—á–∞–µ–º –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º"""
        self.fit(documents)
        return self.transform(documents)


def cosine_similarity_simple(vec1: List[float], vec2: List[float]) -> float:
    """–ü—Ä–æ—Å—Ç–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    if not vec1 or not vec2 or len(vec1) != len(vec2):
        return 0.0
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


class SchemaRetriever:
    """RAG-—Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å—Ö–µ–º–µ –ë–î"""
    
    def __init__(self, schema_file: str = "schema.json"):
        self.schema_file = schema_file
        self.schema_data: Dict[str, Any] = {}
        self.business_dict = BusinessDictionary()
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.table_index: Dict[str, Dict[str, Any]] = {}
        self.column_index: Dict[str, Dict[str, Any]] = {}
        self.relationship_index: List[Dict[str, Any]] = []
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        self.vectorizer = None
        self.document_vectors: List[List[float]] = []
        self.documents: List[str] = []
        self.document_metadata: List[Dict[str, Any]] = []
        
        self._load_schema()
        self._build_indexes()
        self._build_vector_index()
    
    def _load_schema(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ö–µ–º—É –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(self.schema_file, 'r', encoding='utf-8') as f:
                self.schema_data = json.load(f)
            logger.info(f"Schema loaded from {self.schema_file}")
        except FileNotFoundError:
            logger.warning(f"Schema file {self.schema_file} not found. Using empty schema.")
            self.schema_data = {"tables": {}, "fks": [], "schemas": []}
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing schema file: {e}")
            self.schema_data = {"tables": {}, "fks": [], "schemas": []}
    
    def _build_indexes(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –ò–Ω–¥–µ–∫—Å —Ç–∞–±–ª–∏—Ü
        for table_name, table_info in self.schema_data.get("tables", {}).items():
            self.table_index[table_name] = {
                "name": table_name,
                "description": table_info.get("description", ""),
                "columns": [col["name"] for col in table_info.get("columns", [])],
                "column_count": len(table_info.get("columns", [])),
                "row_count": table_info.get("row_count", 0),
                "schema": table_info.get("schema", "public")
            }
        
        # –ò–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–æ–∫
        for table_name, table_info in self.schema_data.get("tables", {}).items():
            for column in table_info.get("columns", []):
                column_key = f"{table_name}.{column['name']}"
                self.column_index[column_key] = {
                    "table": table_name,
                    "name": column["name"],
                    "type": column["type"],
                    "tags": column.get("tags", []),
                    "description": column.get("description", ""),
                    "pk": column.get("pk", False),
                    "nullable": column.get("nullable", True)
                }
        
        # –ò–Ω–¥–µ–∫—Å —Å–≤—è–∑–µ–π
        for fk in self.schema_data.get("fks", []):
            self.relationship_index.append({
                "from": fk["from"],
                "to": fk["to"],
                "constraint": fk.get("constraint", ""),
                "type": "foreign_key"
            })
        
        logger.info(f"Built indexes: {len(self.table_index)} tables, {len(self.column_index)} columns, {len(self.relationship_index)} relationships")
    
    def _build_vector_index(self):
        """–°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        documents = []
        metadata = []
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü
        for table_name, table_info in self.table_index.items():
            doc_text = f"table {table_name} {table_info['description']} columns: {' '.join(table_info['columns'])}"
            documents.append(doc_text)
            metadata.append({
                "type": "table",
                "name": table_name,
                "data": table_info
            })
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫
        for column_key, column_info in self.column_index.items():
            tags_text = ' '.join(column_info['tags'])
            doc_text = f"column {column_info['name']} {column_info['type']} {column_info['description']} tags: {tags_text}"
            documents.append(doc_text)
            metadata.append({
                "type": "column",
                "name": column_key,
                "data": column_info
            })
        
        # –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–æ–≤
        for term_key, term in self.business_dict.terms.items():
            if term in [m["data"] for m in metadata if m.get("data") == term]:
                continue  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            
            synonyms_text = ' '.join(term.synonyms)
            examples_text = ' '.join(term.examples)
            doc_text = f"business term {term.term} {term.definition} {synonyms_text} {examples_text} category: {term.category}"
            documents.append(doc_text)
            metadata.append({
                "type": "business_term",
                "name": term.term,
                "data": term
            })
        
        self.documents = documents
        self.document_metadata = metadata
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º sklearn –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        if HAS_SKLEARN:
            try:
                self.vectorizer = TfidfVectorizer(max_features=1000, stop_words=None, lowercase=True)
                vectors_matrix = self.vectorizer.fit_transform(documents)
                self.document_vectors = vectors_matrix.toarray().tolist()
                logger.info("Using sklearn TfidfVectorizer for vector index")
            except Exception as e:
                logger.warning(f"sklearn vectorizer failed: {e}. Using simple vectorizer.")
                self._build_simple_vector_index()
        else:
            self._build_simple_vector_index()
        
        logger.info(f"Built vector index with {len(documents)} documents")
    
    def _build_simple_vector_index(self):
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        self.vectorizer = SimpleVectorizer(max_features=1000)
        self.document_vectors = self.vectorizer.fit_transform(self.documents)
        logger.info("Using simple TfidfVectorizer for vector index")
    
    def search_tables(self, query: str, limit: int = 5) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        results = []
        query_lower = query.lower()
        
        for table_name, table_info in self.table_index.items():
            score = 0.0
            
            # –ü–æ–∏—Å–∫ –≤ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã
            if query_lower in table_name.lower():
                score += 1.0
            
            # –ü–æ–∏—Å–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
            if table_info['description'] and query_lower in table_info['description'].lower():
                score += 0.8
            
            # –ü–æ–∏—Å–∫ –≤ –∏–º–µ–Ω–∞—Ö –∫–æ–ª–æ–Ω–æ–∫
            matching_columns = [col for col in table_info['columns'] if query_lower in col.lower()]
            if matching_columns:
                score += 0.6 * len(matching_columns) / len(table_info['columns'])
            
            if score > 0:
                results.append(SearchResult(
                    content=f"Table: {table_name}",
                    relevance_score=score,
                    result_type="table",
                    metadata=table_info
                ))
        
        return sorted(results, key=lambda x: x.relevance_score, reverse=True)[:limit]
    
    def search_columns(self, query: str, limit: int = 10) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        results = []
        query_lower = query.lower()
        
        for column_key, column_info in self.column_index.items():
            score = 0.0
            
            # –ü–æ–∏—Å–∫ –≤ –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏
            if query_lower in column_info['name'].lower():
                score += 1.0
            
            # –ü–æ–∏—Å–∫ –≤ —Ç–∏–ø–µ
            if query_lower in column_info['type'].lower():
                score += 0.7
            
            # –ü–æ–∏—Å–∫ –≤ —Ç–µ–≥–∞—Ö
            matching_tags = [tag for tag in column_info['tags'] if query_lower in tag.lower()]
            if matching_tags:
                score += 0.8 * len(matching_tags) / len(column_info['tags']) if column_info['tags'] else 0
            
            # –ü–æ–∏—Å–∫ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
            if column_info['description'] and query_lower in column_info['description'].lower():
                score += 0.6
            
            if score > 0:
                results.append(SearchResult(
                    content=f"Column: {column_key} ({column_info['type']})",
                    relevance_score=score,
                    result_type="column",
                    metadata=column_info
                ))
        
        return sorted(results, key=lambda x: x.relevance_score, reverse=True)[:limit]
    
    def search_relationships(self, query: str, limit: int = 5) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        results = []
        query_lower = query.lower()
        
        for rel in self.relationship_index:
            score = 0.0
            
            # –ü–æ–∏—Å–∫ –≤ –∏–º–µ–Ω–∞—Ö —Ç–∞–±–ª–∏—Ü
            if any(query_lower in table.lower() for table in [rel['from'], rel['to']]):
                score += 1.0
            
            # –ü–æ–∏—Å–∫ –≤ –∏–º–µ–Ω–∞—Ö –∫–æ–ª–æ–Ω–æ–∫
            from_parts = rel['from'].split('.')
            to_parts = rel['to'].split('.')
            
            if len(from_parts) > 1 and query_lower in from_parts[-1].lower():
                score += 0.8
            if len(to_parts) > 1 and query_lower in to_parts[-1].lower():
                score += 0.8
            
            if score > 0:
                results.append(SearchResult(
                    content=f"Relationship: {rel['from']} -> {rel['to']}",
                    relevance_score=score,
                    result_type="relationship",
                    metadata=rel
                ))
        
        return sorted(results, key=lambda x: x.relevance_score, reverse=True)[:limit]
    
    def semantic_search(self, query: str, limit: int = 10, min_score: float = 0.1) -> List[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É –∏–Ω–¥–µ–∫—Å—É"""
        if not self.vectorizer or not self.document_vectors:
            logger.warning("Vector index not available. Falling back to keyword search.")
            return self.keyword_search(query, limit)
        
        try:
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            if HAS_SKLEARN and hasattr(self.vectorizer, 'transform'):
                query_vector = self.vectorizer.transform([query]).toarray()[0].tolist()
            else:
                query_vector = self.vectorizer.transform([query])[0]
            
            results = []
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–∞–∂–¥—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
            for i, doc_vector in enumerate(self.document_vectors):
                if HAS_SKLEARN and HAS_NUMPY:
                    similarity = cosine_similarity([query_vector], [doc_vector])[0][0]
                else:
                    similarity = cosine_similarity_simple(query_vector, doc_vector)
                
                if similarity >= min_score:
                    metadata = self.document_metadata[i]
                    
                    content = f"{metadata['type'].title()}: {metadata['name']}"
                    if metadata['type'] == 'column':
                        content += f" ({metadata['data']['type']})"
                    elif metadata['type'] == 'business_term':
                        content += f" - {metadata['data'].definition}"
                    
                    results.append(SearchResult(
                        content=content,
                        relevance_score=similarity,
                        result_type=metadata['type'],
                        metadata=metadata['data']
                    ))
            
            return sorted(results, key=lambda x: x.relevance_score, reverse=True)[:limit]
        
        except Exception as e:
            logger.error(f"Semantic search failed: {e}. Falling back to keyword search.")
            return self.keyword_search(query, limit)
    
    def keyword_search(self, query: str, limit: int = 10) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (fallback)"""
        all_results = []
        
        # –ü–æ–∏—Å–∫ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö
        all_results.extend(self.search_tables(query, limit // 2))
        
        # –ü–æ–∏—Å–∫ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
        all_results.extend(self.search_columns(query, limit // 2))
        
        # –ü–æ–∏—Å–∫ –≤ —Å–≤—è–∑—è—Ö
        all_results.extend(self.search_relationships(query, limit // 4))
        
        # –ü–æ–∏—Å–∫ –≤ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω–∞—Ö
        business_results = self.business_dict.search_terms(query)
        for term, score in business_results[:limit // 4]:
            all_results.append(SearchResult(
                content=f"Business term: {term.term} - {term.definition}",
                relevance_score=score,
                result_type="business_term",
                metadata=term.__dict__
            ))
        
        return sorted(all_results, key=lambda x: x.relevance_score, reverse=True)[:limit]
    
    def search(self, query: str, search_type: str = "semantic", limit: int = 10) -> List[SearchResult]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫"""
        if search_type == "semantic":
            return self.semantic_search(query, limit)
        elif search_type == "tables":
            return self.search_tables(query, limit)
        elif search_type == "columns":
            return self.search_columns(query, limit)
        elif search_type == "relationships":
            return self.search_relationships(query, limit)
        else:
            return self.keyword_search(query, limit)
    
    def get_table_context(self, table_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã"""
        if table_name not in self.table_index:
            return {}
        
        table_info = self.table_index[table_name]
        
        # –ò—â–µ–º —Å–≤—è–∑–∏
        related_tables = []
        for rel in self.relationship_index:
            if table_name in rel['from']:
                related_tables.append(rel['to'].split('.')[1] if '.' in rel['to'] else rel['to'])
            elif table_name in rel['to']:
                related_tables.append(rel['from'].split('.')[1] if '.' in rel['from'] else rel['from'])
        
        # –ò—â–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã
        related_terms = []
        for term_key, term in self.business_dict.terms.items():
            if table_name.lower() in term.related_tables or any(col in term.related_columns for col in table_info['columns']):
                if term not in related_terms:
                    related_terms.append(term)
        
        return {
            "table_info": table_info,
            "related_tables": list(set(related_tables)),
            "related_terms": [term.__dict__ for term in related_terms[:5]],
            "columns": [self.column_index.get(f"{table_name}.{col}", {}) for col in table_info['columns']]
        }
    
    def suggest_joins(self, tables: List[str]) -> List[Dict[str, str]]:
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã JOIN –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        joins = []
        
        for i, table1 in enumerate(tables):
            for table2 in tables[i+1:]:
                # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å–≤—è–∑–∏
                for rel in self.relationship_index:
                    if (table1 in rel['from'] and table2 in rel['to']) or \
                       (table2 in rel['from'] and table1 in rel['to']):
                        joins.append({
                            "from_table": table1,
                            "to_table": table2,
                            "join_condition": f"{rel['from']} = {rel['to']}",
                            "join_type": "INNER"
                        })
        
        return joins


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Schema Retriever Test')
    parser.add_argument('--schema', type=str, default='schema.json', help='Schema JSON file')
    parser.add_argument('--query', type=str, required=True, help='Search query')
    parser.add_argument('--type', type=str, default='semantic', 
                       choices=['semantic', 'tables', 'columns', 'relationships', 'keyword'],
                       help='Search type')
    parser.add_argument('--limit', type=int, default=10, help='Maximum number of results')
    
    args = parser.parse_args()
    
    logging.basicConfig(level=logging.INFO)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä
    retriever = SchemaRetriever(args.schema)
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
    results = retriever.search(args.query, args.type, args.limit)
    
    print(f"üîç Search query: '{args.query}'")
    print(f"üìã Search type: {args.type}")
    print(f"üìä Found {len(results)} results:\n")
    
    for i, result in enumerate(results, 1):
        print(f"{i}. {result.content}")
        print(f"   Score: {result.relevance_score:.3f}")
        print(f"   Type: {result.result_type}")
        if result.result_type == "table":
            print(f"   Columns: {result.metadata.get('column_count', 0)}")
        elif result.result_type == "column":
            print(f"   Table: {result.metadata.get('table', 'unknown')}")
            print(f"   Tags: {', '.join(result.metadata.get('tags', []))}")
        print()
