"""
Fine-tuned SQL Generator –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Phi-3 + LoRA –º–æ–¥–µ–ª–∏
–ë–µ–∑ API —Å–µ—Ä–≤–µ—Ä–∞, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é —Å –º–æ–¥–µ–ª—å—é
"""

import os
import time
import torch
from typing import Tuple, Dict, List
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º BusinessDictionary –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class BusinessDictionary:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –±–∏–∑–Ω–µ—Å-—Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.terms = {
            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            '–ø—Ä–∏–±—ã–ª—å': 'revenue - costs',
            '–º–∞—Ä–∂–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å': '(revenue - costs) / revenue * 100',
            '—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫': 'AVG(order_amount)',
            '–≤—ã—Ä—É—á–∫–∞': 'SUM(revenue)',
            '–æ—Å—Ç–∞—Ç–∫–∏': 'current_stock',
            '–æ–±–æ—Ä–æ—Ç': 'SUM(turnover)',
            '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å': '(profit / revenue) * 100',
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
            '—Å–µ–≥–æ–¥–Ω—è': 'DATE(created_at) = CURRENT_DATE',
            '–≤—á–µ—Ä–∞': 'DATE(created_at) = CURRENT_DATE - 1',
            '–∑–∞ –Ω–µ–¥–µ–ª—é': 'created_at >= CURRENT_DATE - INTERVAL \'7 days\'',
            '–∑–∞ –º–µ—Å—è—Ü': 'created_at >= CURRENT_DATE - INTERVAL \'30 days\'',
            '–∑–∞ –∫–≤–∞—Ä—Ç–∞–ª': 'created_at >= CURRENT_DATE - INTERVAL \'90 days\'',
            '–∑–∞ –≥–æ–¥': 'created_at >= CURRENT_DATE - INTERVAL \'365 days\'',
            
            # –¢–∞–±–ª–∏—Ü—ã –∏ –ø–æ–ª—è
            '–∑–∞–∫–∞–∑—ã': 'orders',
            '–∫–ª–∏–µ–Ω—Ç—ã': 'customers', 
            '—Ç–æ–≤–∞—Ä—ã': 'products',
            '–ø—Ä–æ–¥–∞–∂–∏': 'sales',
            '—Å–∫–ª–∞–¥': 'inventory',
            '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏': 'employees'
        }
        
    def translate_term(self, term: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω –≤ SQL –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é"""
        term_lower = term.lower().strip()
        return self.terms.get(term_lower, term)
    
    def get_related_terms(self, query: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –±–∏–∑–Ω–µ—Å-—Ç–µ—Ä–º–∏–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        found_terms = []
        query_lower = query.lower()
        for term in self.terms.keys():
            if term in query_lower:
                found_terms.append(term)
        return found_terms


class FineTunedSQLGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SQL –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fine-tuned Phi-3 + LoRA –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path: str = "finetuning/phi3-mini", adapter_path: str = "finetuning/phi3_bird_lora"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fine-tuned –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ü—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Phi-3
            adapter_path: –ü—É—Ç—å –∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä—É
        """
        self.model_path = Path(model_path)
        self.adapter_path = Path(adapter_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∞–¥–∞–ø—Ç–µ—Ä–∞
        if not self.model_path.exists():
            raise FileNotFoundError(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
        
        if not self.adapter_path.exists():
            raise FileNotFoundError(f"LoRA –∞–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.adapter_path}")
        
        print(f"üîß –ó–∞–≥—Ä—É–∂–∞–µ–º fine-tuned –º–æ–¥–µ–ª—å...")
        print(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {self.model_path}")
        print(f"   LoRA –∞–¥–∞–ø—Ç–µ—Ä: {self.adapter_path}")
        
        self._load_model()
        
        # –î–æ–±–∞–≤–ª—è–µ–º business_dict –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å BIGPTAgent
        self.business_dict = BusinessDictionary()
        
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∞–¥–∞–ø—Ç–µ—Ä"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            print("   üìù –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path), use_fast=True)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            print("   üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å...")
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.float16 if torch.backends.mps.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True,
                attn_implementation="eager"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º eager attention –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä
            print("   üîó –ü–æ–¥–∫–ª—é—á–∞–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä...")
            self.model = PeftModel.from_pretrained(self.model, str(self.adapter_path))
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            self.model.eval()
            
            print("   ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def generate_sql(self, user_query: str, schema_info: Dict = None) -> Tuple[str, float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç SQL –∑–∞–ø—Ä–æ—Å –∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
        
        Args:
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
            schema_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ö–µ–º–µ –ë–î (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏)
            
        Returns:
            Tuple[str, float]: (SQL –∑–∞–ø—Ä–æ—Å, –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)
        """
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
        prompt = self._create_prompt(user_query)
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1024
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            device = next(self.model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with torch.no_grad():
                try:
                    outputs = self.model.generate(
                        inputs['input_ids'],
                        attention_mask=inputs.get('attention_mask'),
                        max_new_tokens=80,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                        do_sample=False,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=False,
                        # –î–æ–±–∞–≤–ª—è–µ–º stop tokens –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                        early_stopping=True
                    )
                except Exception as cache_error:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å –∫—ç—à–µ–º, –ø—Ä–æ–±—É–µ–º –±–µ–∑ attention_mask: {cache_error}")
                    # Fallback –±–µ–∑ attention_mask
                    outputs = self.model.generate(
                        inputs['input_ids'],
                        max_new_tokens=80,  # –£–º–µ–Ω—å—à–∏–ª–∏
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=False,
                        early_stopping=True
                    )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–±–µ–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞)
            input_length = inputs['input_ids'].shape[1]
            new_tokens = outputs[0][input_length:]
            generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
            
            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            print(f"üìù –ù–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞): {generated_text}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ SQL –∏–∑ –æ—Ç–≤–µ—Ç–∞ (—Ç–µ–ø–µ—Ä—å –±–µ–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞)
            sql_query = self._extract_sql_from_generated(generated_text)
            
            execution_time = time.time() - start_time
            
            if sql_query:
                print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π SQL: {sql_query}")
            else:
                print("‚ùå SQL –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å")
            
            return sql_query, execution_time
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL: {e}")
            import traceback
            traceback.print_exc()  # –ü–µ—á–∞—Ç–∞–µ–º –ø–æ–ª–Ω—ã–π —Å—Ç–µ–∫—Ç—Ä–µ–π—Å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            return "", time.time() - start_time
    
    def _extract_sql_from_generated(self, generated_text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç SQL –∏–∑ —É–∂–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞)"""
        try:
            sql_part = generated_text.strip()
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞—Ö (–º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã)
            stop_words = [
                'Question:', 'SQL:', 'Database:', 'Schema:', 'Answer:', 'Explanation:', 
                '\n\nQuestion', '\n\nDatabase', '\n\nSchema', '\nQuestion', '\nDatabase',
                'Question', 'Database'  # –î–∞–∂–µ –±–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏—è
            ]
            
            for stop_word in stop_words:
                if stop_word in sql_part:
                    sql_part = sql_part.split(stop_word)[0].strip()
                    break
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ–Ω–æ—Å—ã
            if '\n' in sql_part:
                lines = sql_part.split('\n')
                # –ò—â–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –∫–æ—Ç–æ—Ä–∞—è –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ SQL
                for line in lines:
                    line = line.strip()
                    if line and line.upper().startswith('SELECT'):
                        sql_part = line
                        break
                else:
                    sql_part = lines[0].strip()
            
            # –£–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ
            if sql_part.endswith(';'):
                sql_part = sql_part[:-1]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π SQL
            if not sql_part.upper().startswith('SELECT'):
                print(f"‚ö†Ô∏è  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç SELECT: {sql_part[:50]}...")
                return ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –º—É—Å–æ—Ä–∞
            invalid_keywords = ['Question', 'Database', 'Schema', 'Answer', 'Explanation']
            for keyword in invalid_keywords:
                if keyword in sql_part:
                    print(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω –º—É—Å–æ—Ä –≤ SQL: {keyword}")
                    return ""
            
            # –î–æ–±–∞–≤–ª—è–µ–º LIMIT –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'LIMIT' not in sql_part.upper():
                sql_part += ' LIMIT 1000'
            
            return sql_part
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SQL –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    def _create_prompt(self, user_query: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º PostgreSQL —Å—Ö–µ–º—É
        schema = """
customers: id (SERIAL), name (VARCHAR), email (VARCHAR), registration_date (DATE), segment (VARCHAR)
products: id (SERIAL), name (VARCHAR), category (VARCHAR), price (DECIMAL), cost (DECIMAL)  
orders: id (SERIAL), customer_id (INTEGER), order_date (DATE), amount (DECIMAL), status (VARCHAR)
sales: id (SERIAL), order_id (INTEGER), product_id (INTEGER), quantity (INTEGER), revenue (DECIMAL), costs (DECIMAL)
inventory: id (SERIAL), product_id (INTEGER), current_stock (INTEGER), warehouse (VARCHAR)
"""
        
        prompt = f"""You are a helpful data analyst that writes clean PostgreSQL.
Return ONLY SQL without comments or explanations.

Database: bi_demo
Schema:
{schema.strip()}

Question: {user_query}
SQL:"""
        
        return prompt
    
    def _extract_sql(self, generated_text: str, original_prompt: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç SQL –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            if original_prompt in generated_text:
                sql_part = generated_text.split(original_prompt, 1)[1].strip()
            else:
                sql_part = generated_text.strip()
            
            # –ò—â–µ–º SQL –ø–æ—Å–ª–µ "SQL:" - –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            if 'SQL:' in sql_part:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ SQL:
                sql_part = sql_part.split('SQL:', 1)[1].strip()
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞—Ö (–≤–∫–ª—é—á–∞—è –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã)
            stop_words = [
                'Question:', 'SQL:', 'Database:', 'Schema:', 'Answer:', 'Explanation:', 
                '\n\n', '\nQuestion', '\nDatabase', '\nSchema', 'Question', 'Database'
            ]
            
            for stop_word in stop_words:
                if stop_word in sql_part:
                    sql_part = sql_part.split(stop_word)[0].strip()
                    break  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø–µ—Ä–≤–æ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–º
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
            sql_part = sql_part.strip()
            
            # –£–±–∏—Ä–∞–µ–º —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ
            if sql_part.endswith(';'):
                sql_part = sql_part[:-1]
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ–Ω–æ—Å
            if '\n' in sql_part:
                sql_part = sql_part.split('\n')[0].strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π SQL
            if not sql_part.upper().startswith('SELECT'):
                print(f"‚ö†Ô∏è  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç SELECT: {sql_part[:100]}...")
                return ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –º—É—Å–æ—Ä–∞
            invalid_keywords = ['Question', 'Database', 'Schema', 'Answer']
            for keyword in invalid_keywords:
                if keyword in sql_part:
                    print(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω –º—É—Å–æ—Ä –≤ SQL: {keyword}")
                    return ""
            
            # –î–æ–±–∞–≤–ª—è–µ–º LIMIT –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'LIMIT' not in sql_part.upper():
                sql_part += ' LIMIT 1000'
            
            return sql_part
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è SQL: {e}")
            return ""
    
    def cleanup(self):
        """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã –º–æ–¥–µ–ª–∏"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()


def test_finetuned_generator():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç fine-tuned –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º fine-tuned SQL –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä...")
    
    try:
        generator = FineTunedSQLGenerator()
        
        test_queries = [
            "–ø–æ–∫–∞–∂–∏ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤",
            "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤",
            "—Å—Ä–µ–¥–Ω–∏–π —á–µ–∫ –∫–ª–∏–µ–Ω—Ç–æ–≤",
            "—Ç–æ–ø 3 –∫–ª–∏–µ–Ω—Ç–∞ –ø–æ –≤—ã—Ä—É—á–∫–µ"
        ]
        
        for query in test_queries:
            print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
            sql, exec_time = generator.generate_sql(query)
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {exec_time:.2f}—Å")
            print(f"üîç SQL: {sql}")
        
        generator.cleanup()
        print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")


if __name__ == "__main__":
    test_finetuned_generator()
