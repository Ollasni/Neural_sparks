"""
Natural Language Normalizer for BI-GPT Agent
ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹, Ğ´Ğ°Ñ‚Ñ‹, Ñ‡Ğ¸ÑĞ»Ğ°
ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾, ĞºĞ°Ğ·Ğ°Ñ…ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²
"""

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta, date
from enum import Enum
import calendar

logger = logging.getLogger(__name__)


class Language(Enum):
    """ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸"""
    RUSSIAN = "ru"
    KAZAKH = "kz" 
    ENGLISH = "en"


@dataclass
class NormalizedQuery:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"""
    original: str
    normalized: str
    detected_language: Language
    extracted_dates: List[Dict[str, Any]] = field(default_factory=list)
    extracted_numbers: List[Dict[str, Any]] = field(default_factory=list)
    business_terms: List[str] = field(default_factory=list)
    intent: Optional[str] = None
    confidence: float = 1.0


class LanguageDetector:
    """ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ°"""
    
    def __init__(self):
        self.patterns = {
            Language.RUSSIAN: [
                r'\b(Ğ¿Ğ¾ĞºĞ°Ğ¶Ğ¸|Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ|Ğ²Ñ‹Ğ²ĞµÑÑ‚Ğ¸|Ğ½Ğ°Ğ¹Ñ‚Ğ¸|Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ|Ğ´Ğ°Ğ¹|Ğ´Ğ°Ğ¹Ñ‚Ğµ)\b',
                r'\b(ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹|Ğ·Ğ°ĞºĞ°Ğ·Ñ‹|Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸|Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹|Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ|Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°)\b',
                r'\b(Ğ·Ğ°|Ğ¿Ğ¾|Ğ´Ğ»Ñ|Ñ|Ğ²|Ğ½Ğ°|Ğ¾Ñ‚|Ğ´Ğ¾)\b',
                r'\b(ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ|Ğ²Ñ‡ĞµÑ€Ğ°|Ğ½ĞµĞ´ĞµĞ»Ñ|Ğ¼ĞµÑÑÑ†|Ğ³Ğ¾Ğ´)\b'
            ],
            Language.KAZAKH: [
                r'\b(ĞºÓ©Ñ€ÑĞµÑ‚|Ñ‚Ğ°Ğ±Ñƒ|Ğ°Ğ»Ñƒ|Ğ±ĞµÑ€Ñƒ)\b',
                r'\b(ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‚ĞµÑ€|Ñ‚Ğ°Ğ¿ÑÑ‹Ñ€Ñ‹ÑÑ‚Ğ°Ñ€|ÑĞ°Ñ‚ÑƒĞ»Ğ°Ñ€|Ñ‚Ğ°ÑƒĞ°Ñ€Ğ»Ğ°Ñ€)\b',
                r'\b(Ò¯ÑˆÑ–Ğ½|Ğ±Ğ¾Ğ¹Ñ‹Ğ½ÑˆĞ°|Ğ´ĞµĞ¹Ñ–Ğ½|ĞºĞµĞ¹Ñ–Ğ½)\b'
            ],
            Language.ENGLISH: [
                r'\b(show|get|find|display|list|select)\b',
                r'\b(customers|orders|sales|products|revenue|profit)\b',
                r'\b(for|by|from|to|with|in|on)\b',
                r'\b(today|yesterday|week|month|year)\b'
            ]
        }
    
    def detect(self, text: str) -> Language:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑĞ·Ñ‹Ğº Ñ‚ĞµĞºÑÑ‚Ğ°"""
        text_lower = text.lower()
        scores = {}
        
        for lang, patterns in self.patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, text_lower))
                score += matches
            scores[lang] = score
        
        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ÑĞ·Ñ‹Ğº Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑÑ‡ĞµÑ‚Ğ¾Ğ¼
        if scores:
            detected_lang = max(scores, key=scores.get)
            if scores[detected_lang] > 0:
                return detected_lang
        
        # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ñ€ÑƒÑÑĞºĞ¸Ğ¹
        return Language.RUSSIAN


class SynonymNormalizer:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²"""
    
    def __init__(self):
        self.synonyms = {
            Language.RUSSIAN: {
                # Ğ“Ğ»Ğ°Ğ³Ğ¾Ğ»Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹
                'Ğ¿Ğ¾ĞºĞ°Ğ¶Ğ¸': ['Ğ¿Ğ¾ĞºĞ°Ğ¶Ğ¸', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ', 'Ğ²Ñ‹Ğ²ĞµÑÑ‚Ğ¸', 'Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸', 'Ğ´Ğ°Ğ¹', 'Ğ´Ğ°Ğ¹Ñ‚Ğµ', 'Ğ²Ñ‹Ğ²ĞµĞ´Ğ¸'],
                'Ğ½Ğ°Ğ¹Ğ´Ğ¸': ['Ğ½Ğ°Ğ¹Ğ´Ğ¸', 'Ğ½Ğ°Ğ¹Ñ‚Ğ¸', 'Ğ¾Ñ‚Ñ‹Ñ‰Ğ¸', 'Ğ¸Ñ‰Ğ¸', 'Ğ¿Ğ¾Ğ¸ÑĞº'],
                'Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸': ['Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸', 'Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ', 'Ğ²Ğ·ÑÑ‚ÑŒ', 'Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ'],
                'Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸': ['Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸', 'Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ', 'Ğ¾Ñ‚Ğ±ĞµÑ€Ğ¸', 'Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ', 'Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞ¹'],
                
                # Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸
                'ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹': ['ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹', 'Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»Ğ¸', 'Ğ·Ğ°ĞºĞ°Ğ·Ñ‡Ğ¸ĞºĞ¸', 'Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»Ğ¸', 'Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸', 'ÑĞ·ĞµÑ€Ñ‹'],
                'Ğ·Ğ°ĞºĞ°Ğ·Ñ‹': ['Ğ·Ğ°ĞºĞ°Ğ·Ñ‹', 'Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸', 'ÑĞ´ĞµĞ»ĞºĞ¸', 'Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¸', 'Ğ¾Ñ€Ğ´ĞµÑ€Ğ°'],
                'Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹': ['Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹', 'Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹', 'Ğ¸Ğ·Ğ´ĞµĞ»Ğ¸Ñ', 'Ğ½Ğ¾Ğ¼ĞµĞ½ĞºĞ»Ğ°Ñ‚ÑƒÑ€Ğ°', 'items', 'Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ†Ğ¸Ñ'],
                'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸': ['Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸', 'Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ', 'ÑĞ±Ñ‹Ñ‚', 'sales'],
                'Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸': ['Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸', 'ÑĞºĞ»Ğ°Ğ´', 'Ğ·Ğ°Ğ¿Ğ°ÑÑ‹', 'Ğ¸Ğ½Ğ²ĞµĞ½Ñ‚Ğ°Ñ€ÑŒ', 'Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº', 'stock'],
                
                # Ğ¤Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸  
                'Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°': ['Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°', 'Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´Ñ‹', 'Ğ¿Ğ¾ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ñ', 'revenue'],
                'Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ': ['Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´', 'Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ñ‚', 'profit', 'Ñ‡Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ'],
                'Ğ¼Ğ°Ñ€Ğ¶Ğ°': ['Ğ¼Ğ°Ñ€Ğ¶Ğ°', 'Ğ¼Ğ°Ñ€Ğ¶Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'Ñ€ĞµĞ½Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ'],
                'ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹_Ñ‡ĞµĞº': ['ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‡ĞµĞº', 'ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ·Ğ°ĞºĞ°Ğ·', 'aov', 'average order value'],
                
                # ĞĞ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸
                'ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾': ['ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'Ñ‡Ğ¸ÑĞ»Ğ¾', 'ĞºĞ¾Ğ»-Ğ²Ğ¾', 'count', 'ÑˆÑ‚ÑƒĞº'],
                'ÑÑƒĞ¼Ğ¼Ğ°': ['ÑÑƒĞ¼Ğ¼Ğ°', 'Ğ¸Ñ‚Ğ¾Ğ³Ğ¾', 'Ğ²ÑĞµĞ³Ğ¾', 'total', 'sum'],
                'ÑÑ€ĞµĞ´Ğ½ĞµĞµ': ['ÑÑ€ĞµĞ´Ğ½ĞµĞµ', 'ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹', 'avg', 'average'],
                'Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼': ['Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼', 'Ğ¼Ğ°ĞºÑ', 'max', 'maximum', 'Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹'],
                'Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼': ['Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼', 'Ğ¼Ğ¸Ğ½', 'min', 'minimum', 'Ğ½Ğ°Ğ¸Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹'],
                
                # Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ñ‹
                'ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ': ['ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ', 'today'],
                'Ğ²Ñ‡ĞµÑ€Ğ°': ['Ğ²Ñ‡ĞµÑ€Ğ°', 'yesterday'],
                'Ğ½ĞµĞ´ĞµĞ»Ñ': ['Ğ½ĞµĞ´ĞµĞ»Ñ', 'week', 'Ğ·Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ', 'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ğ½ĞµĞ´ĞµĞ»Ñ'],
                'Ğ¼ĞµÑÑÑ†': ['Ğ¼ĞµÑÑÑ†', 'month', 'Ğ·Ğ° Ğ¼ĞµÑÑÑ†', 'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¼ĞµÑÑÑ†'],
                'ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»': ['ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»', 'quarter', 'Ğ·Ğ° ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»'],
                'Ğ³Ğ¾Ğ´': ['Ğ³Ğ¾Ğ´', 'year', 'Ğ·Ğ° Ğ³Ğ¾Ğ´', 'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ´'],
                
                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ
                'Ğ³Ğ´Ğµ': ['Ğ³Ğ´Ğµ', 'Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼', 'Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸', 'where'],
                'Ğ±Ğ¾Ğ»ÑŒÑˆĞµ': ['Ğ±Ğ¾Ğ»ÑŒÑˆĞµ', 'Ğ±Ğ¾Ğ»ĞµĞµ', 'ÑĞ²Ñ‹ÑˆĞµ', 'Ğ²Ñ‹ÑˆĞµ', 'greater', 'gt'],
                'Ğ¼ĞµĞ½ÑŒÑˆĞµ': ['Ğ¼ĞµĞ½ÑŒÑˆĞµ', 'Ğ¼ĞµĞ½ĞµĞµ', 'Ğ½Ğ¸Ğ¶Ğµ', 'less', 'lt'],
                'Ñ€Ğ°Ğ²Ğ½Ğ¾': ['Ñ€Ğ°Ğ²Ğ½Ğ¾', 'Ñ€Ğ°Ğ²ĞµĞ½', 'equal', 'eq', '='],
                'Ğ½Ğµ_Ñ€Ğ°Ğ²Ğ½Ğ¾': ['Ğ½Ğµ Ñ€Ğ°Ğ²Ğ½Ğ¾', 'Ğ½Ğµ Ñ€Ğ°Ğ²ĞµĞ½', 'not equal', 'ne', '!='],
                
                # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°
                'Ñ‚Ğ¾Ğ¿': ['Ñ‚Ğ¾Ğ¿', 'Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ', 'top', 'Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ'],
                'Ñ…ÑƒĞ´ÑˆĞ¸Ğµ': ['Ñ…ÑƒĞ´ÑˆĞ¸Ğµ', 'worst', 'bottom'],
                'ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°': ['ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ', 'ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ', 'order by', 'sort']
            },
            
            Language.ENGLISH: {
                # Action verbs
                'show': ['show', 'display', 'list', 'get', 'fetch', 'retrieve'],
                'find': ['find', 'search', 'look for', 'locate'],
                'select': ['select', 'choose', 'pick', 'filter'],
                
                # Business entities
                'customers': ['customers', 'clients', 'users', 'buyers'],
                'orders': ['orders', 'purchases', 'transactions'],
                'products': ['products', 'items', 'goods', 'merchandise'],
                'sales': ['sales', 'revenue'],
                'inventory': ['inventory', 'stock', 'warehouse'],
                
                # Financial metrics
                'revenue': ['revenue', 'income', 'sales', 'turnover'],
                'profit': ['profit', 'earnings', 'net income'],
                'margin': ['margin', 'profitability'],
                'average_order': ['average order', 'aov', 'average order value'],
                
                # Aggregations
                'count': ['count', 'number', 'total number'],
                'sum': ['sum', 'total', 'amount'],
                'average': ['average', 'avg', 'mean'],
                'maximum': ['maximum', 'max', 'highest'],
                'minimum': ['minimum', 'min', 'lowest'],
                
                # Time periods
                'today': ['today'],
                'yesterday': ['yesterday'],
                'week': ['week', 'last week', 'this week'],
                'month': ['month', 'last month', 'this month'],
                'quarter': ['quarter', 'last quarter'],
                'year': ['year', 'last year', 'this year']
            },
            
            Language.KAZAKH: {
                # Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ
                'ĞºÓ©Ñ€ÑĞµÑ‚': ['ĞºÓ©Ñ€ÑĞµÑ‚', 'ĞºÓ©Ñ€ÑĞµÑ‚Ñ–Ò£Ñ–Ğ·', 'ÑˆÑ‹Ò“Ğ°Ñ€'],
                'Ñ‚Ğ°Ğ¿': ['Ñ‚Ğ°Ğ¿', 'Ñ‚Ğ°Ğ±Ñƒ', 'Ñ–Ğ·Ğ´ĞµÑƒ'],
                
                # Ğ¡ÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸
                'ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‚ĞµÑ€': ['ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‚ĞµÑ€', 'ÑĞ°Ñ‚Ñ‹Ğ¿ Ğ°Ğ»ÑƒÑˆÑ‹Ğ»Ğ°Ñ€'],
                'Ñ‚Ğ°Ğ¿ÑÑ‹Ñ€Ñ‹ÑÑ‚Ğ°Ñ€': ['Ñ‚Ğ°Ğ¿ÑÑ‹Ñ€Ñ‹ÑÑ‚Ğ°Ñ€', 'ÑĞ°Ñ‚Ñ‹Ğ¿ Ğ°Ğ»ÑƒĞ»Ğ°Ñ€'],
                'Ñ‚Ğ°ÑƒĞ°Ñ€Ğ»Ğ°Ñ€': ['Ñ‚Ğ°ÑƒĞ°Ñ€Ğ»Ğ°Ñ€', 'Ó©Ğ½Ñ–Ğ¼Ğ´ĞµÑ€'],
                
                # Ğ’Ñ€ĞµĞ¼Ñ
                'Ğ±Ò¯Ğ³Ñ–Ğ½': ['Ğ±Ò¯Ğ³Ñ–Ğ½'],
                'ĞºĞµÑˆĞµ': ['ĞºĞµÑˆĞµ'],
                'Ğ°Ğ¿Ñ‚Ğ°': ['Ğ°Ğ¿Ñ‚Ğ°', 'Ğ°Ğ¿Ñ‚Ğ°Ğ´Ğ°'],
                'Ğ°Ğ¹': ['Ğ°Ğ¹', 'Ğ°Ğ¹Ğ´Ğ°']
            }
        }
    
    def normalize_synonyms(self, text: str, language: Language) -> str:
        """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ"""
        if language not in self.synonyms:
            return text
        
        normalized_text = text.lower()
        
        # ĞŸÑ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ²
        for canonical_form, synonym_list in self.synonyms[language].items():
            for synonym in synonym_list:
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ»Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
                pattern = r'\b' + re.escape(synonym) + r'\b'
                normalized_text = re.sub(pattern, canonical_form, normalized_text, flags=re.IGNORECASE)
        
        return normalized_text


class DateTimeNormalizer:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ°Ñ‚ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸"""
    
    def __init__(self, timezone: str = "Asia/Almaty"):
        self.timezone = timezone
        
        # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²
        self.date_patterns = {
            Language.RUSSIAN: {
                'ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ': 'CURRENT_DATE',
                'Ğ²Ñ‡ĞµÑ€Ğ°': 'CURRENT_DATE - INTERVAL 1 DAY',
                'Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°': 'CURRENT_DATE + INTERVAL 1 DAY',
                'Ğ·Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ': 'CURRENT_DATE - INTERVAL 7 DAY',
                'Ğ·Ğ° Ğ¼ĞµÑÑÑ†': 'CURRENT_DATE - INTERVAL 30 DAY', 
                'Ğ·Ğ° ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»': 'CURRENT_DATE - INTERVAL 90 DAY',
                'Ğ·Ğ° Ğ³Ğ¾Ğ´': 'CURRENT_DATE - INTERVAL 365 DAY',
                'Ğ½ĞµĞ´ĞµĞ»Ñ': 'CURRENT_DATE - INTERVAL 7 DAY',
                'Ğ¼ĞµÑÑÑ†': 'CURRENT_DATE - INTERVAL 30 DAY',
                'ĞºĞ²Ğ°Ñ€Ñ‚Ğ°Ğ»': 'CURRENT_DATE - INTERVAL 90 DAY',
                'Ğ³Ğ¾Ğ´': 'CURRENT_DATE - INTERVAL 365 DAY',
                r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ´Ğ½Ñ?': r'CURRENT_DATE - INTERVAL \1 DAY',
                r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ½ĞµĞ´ĞµĞ»ÑŒ?': r'CURRENT_DATE - INTERVAL \1*7 DAY',
                r'Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ¼ĞµÑÑÑ†ĞµĞ²?': r'CURRENT_DATE - INTERVAL \1*30 DAY',
                r'Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ´Ğ½Ñ?': r'CURRENT_DATE - INTERVAL \1 DAY',
                r'Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ½ĞµĞ´ĞµĞ»ÑŒ?': r'CURRENT_DATE - INTERVAL \1*7 DAY',
                r'Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ (\d+) Ğ¼ĞµÑÑÑ†ĞµĞ²?': r'CURRENT_DATE - INTERVAL \1*30 DAY'
            },
            Language.ENGLISH: {
                'today': 'CURRENT_DATE',
                'yesterday': 'CURRENT_DATE - INTERVAL 1 DAY',
                'tomorrow': 'CURRENT_DATE + INTERVAL 1 DAY',
                'last week': 'CURRENT_DATE - INTERVAL 7 DAY',
                'last month': 'CURRENT_DATE - INTERVAL 30 DAY',
                'last quarter': 'CURRENT_DATE - INTERVAL 90 DAY',
                'last year': 'CURRENT_DATE - INTERVAL 365 DAY',
                r'last (\d+) days?': r'CURRENT_DATE - INTERVAL \1 DAY',
                r'last (\d+) weeks?': r'CURRENT_DATE - INTERVAL \1*7 DAY',
                r'last (\d+) months?': r'CURRENT_DATE - INTERVAL \1*30 DAY'
            },
            Language.KAZAKH: {
                'Ğ±Ò¯Ğ³Ñ–Ğ½': 'CURRENT_DATE',
                'ĞºĞµÑˆĞµ': 'CURRENT_DATE - INTERVAL 1 DAY',
                'ĞµÑ€Ñ‚ĞµÒ£': 'CURRENT_DATE + INTERVAL 1 DAY',
                'Ğ°Ğ¿Ñ‚Ğ°': 'CURRENT_DATE - INTERVAL 7 DAY',
                'Ğ°Ğ¹': 'CURRENT_DATE - INTERVAL 30 DAY'
            }
        }
        
        # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑÑ†ĞµĞ²
        self.months = {
            Language.RUSSIAN: {
                'ÑĞ½Ğ²Ğ°Ñ€ÑŒ': 1, 'ÑĞ½Ğ²Ğ°Ñ€Ñ': 1, 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»ÑŒ': 2, 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ': 2,
                'Ğ¼Ğ°Ñ€Ñ‚': 3, 'Ğ¼Ğ°Ñ€Ñ‚Ğ°': 3, 'Ğ°Ğ¿Ñ€ĞµĞ»ÑŒ': 4, 'Ğ°Ğ¿Ñ€ĞµĞ»Ñ': 4,
                'Ğ¼Ğ°Ğ¹': 5, 'Ğ¼Ğ°Ñ': 5, 'Ğ¸ÑĞ½ÑŒ': 6, 'Ğ¸ÑĞ½Ñ': 6,
                'Ğ¸ÑĞ»ÑŒ': 7, 'Ğ¸ÑĞ»Ñ': 7, 'Ğ°Ğ²Ğ³ÑƒÑÑ‚': 8, 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°': 8,
                'ÑĞµĞ½Ñ‚ÑĞ±Ñ€ÑŒ': 9, 'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ': 9, 'Ğ¾ĞºÑ‚ÑĞ±Ñ€ÑŒ': 10, 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ': 10,
                'Ğ½Ğ¾ÑĞ±Ñ€ÑŒ': 11, 'Ğ½Ğ¾ÑĞ±Ñ€Ñ': 11, 'Ğ´ĞµĞºĞ°Ğ±Ñ€ÑŒ': 12, 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ': 12
            },
            Language.ENGLISH: {
                'january': 1, 'february': 2, 'march': 3, 'april': 4,
                'may': 5, 'june': 6, 'july': 7, 'august': 8,
                'september': 9, 'october': 10, 'november': 11, 'december': 12,
                'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,
                'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,
                'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12
            }
        }
    
    def extract_dates(self, text: str, language: Language) -> List[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        extracted_dates = []
        
        if language not in self.date_patterns:
            return extracted_dates
        
        patterns = self.date_patterns[language]
        
        for pattern, sql_expression in patterns.items():
            if isinstance(pattern, str):
                # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
                if pattern in text.lower():
                    extracted_dates.append({
                        'original': pattern,
                        'sql_expression': sql_expression,
                        'type': 'relative_date'
                    })
            else:
                # Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                matches = re.finditer(pattern, text.lower())
                for match in matches:
                    # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ² SQL Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸
                    sql_expr = sql_expression
                    for i, group in enumerate(match.groups(), 1):
                        sql_expr = sql_expr.replace(f'\\{i}', group)
                    
                    extracted_dates.append({
                        'original': match.group(),
                        'sql_expression': sql_expr,
                        'type': 'relative_date_with_number'
                    })
        
        # Ğ˜Ñ‰ĞµĞ¼ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ñ‹ (DD.MM.YYYY, YYYY-MM-DD)
        absolute_date_patterns = [
            r'\b(\d{1,2})\.(\d{1,2})\.(\d{4})\b',  # DD.MM.YYYY
            r'\b(\d{4})-(\d{1,2})-(\d{1,2})\b',   # YYYY-MM-DD
            r'\b(\d{1,2})/(\d{1,2})/(\d{4})\b'    # DD/MM/YYYY
        ]
        
        for pattern in absolute_date_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                if pattern.endswith(r'\.(\d{4})\b'):  # DD.MM.YYYY
                    day, month, year = match.groups()
                elif pattern.endswith(r'-(\d{1,2})\b'):  # YYYY-MM-DD
                    year, month, day = match.groups()
                else:  # DD/MM/YYYY
                    day, month, year = match.groups()
                
                try:
                    # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ
                    date_obj = datetime(int(year), int(month), int(day))
                    sql_date = f"'{date_obj.strftime('%Y-%m-%d')}'"
                    
                    extracted_dates.append({
                        'original': match.group(),
                        'sql_expression': sql_date,
                        'type': 'absolute_date',
                        'parsed_date': date_obj.isoformat()
                    })
                except ValueError:
                    # ĞĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼
                    continue
        
        return extracted_dates
    
    def normalize_dates(self, text: str, language: Language) -> str:
        """Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‹ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½Ğ° SQL Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ"""
        normalized_text = text
        extracted_dates = self.extract_dates(text, language)
        
        # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¼ĞµĞ½
        for date_info in sorted(extracted_dates, key=lambda x: len(x['original']), reverse=True):
            normalized_text = normalized_text.replace(
                date_info['original'], 
                f"[DATE:{date_info['sql_expression']}]"
            )
        
        return normalized_text


class NumberNormalizer:
    """ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ‡Ğ¸ÑĞµĞ» Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹"""
    
    def __init__(self):
        # Ğ¡Ğ»Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        self.number_words = {
            Language.RUSSIAN: {
                'Ğ¾Ğ´Ğ¸Ğ½': 1, 'Ğ¾Ğ´Ğ½Ğ°': 1, 'Ğ´Ğ²Ğ°': 2, 'Ğ´Ğ²Ğµ': 2, 'Ñ‚Ñ€Ğ¸': 3, 'Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ': 4, 'Ğ¿ÑÑ‚ÑŒ': 5,
                'ÑˆĞµÑÑ‚ÑŒ': 6, 'ÑĞµĞ¼ÑŒ': 7, 'Ğ²Ğ¾ÑĞµĞ¼ÑŒ': 8, 'Ğ´ĞµĞ²ÑÑ‚ÑŒ': 9, 'Ğ´ĞµÑÑÑ‚ÑŒ': 10,
                'Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 11, 'Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 12, 'Ñ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 13, 'Ñ‡ĞµÑ‚Ñ‹Ñ€Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 14, 'Ğ¿ÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 15,
                'Ğ´Ğ²Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ': 20, 'Ñ‚Ñ€Ğ¸Ğ´Ñ†Ğ°Ñ‚ÑŒ': 30, 'ÑĞ¾Ñ€Ğ¾Ğº': 40, 'Ğ¿ÑÑ‚ÑŒĞ´ĞµÑÑÑ‚': 50,
                'ÑÑ‚Ğ¾': 100, 'Ñ‚Ñ‹ÑÑÑ‡Ğ°': 1000, 'Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½': 1000000
            },
            Language.ENGLISH: {
                'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5,
                'six': 6, 'seven': 7, 'eight': 8, 'nine': 9, 'ten': 10,
                'eleven': 11, 'twelve': 12, 'thirteen': 13, 'fourteen': 14, 'fifteen': 15,
                'twenty': 20, 'thirty': 30, 'forty': 40, 'fifty': 50,
                'hundred': 100, 'thousand': 1000, 'million': 1000000
            }
        }
    
    def extract_numbers(self, text: str, language: Language) -> List[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        extracted_numbers = []
        
        # Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        digit_patterns = [
            r'\b(\d+(?:\.\d+)?)\s*(?:Ñ‚Ñ‹Ñ|Ñ‚Ñ‹ÑÑÑ‡|thousand|k)\b',  # Ğ¢Ñ‹ÑÑÑ‡Ğ¸
            r'\b(\d+(?:\.\d+)?)\s*(?:Ğ¼Ğ»Ğ½|Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ²?|million|m)\b',  # ĞœĞ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹
            r'\b(\d+(?:\.\d+)?)\s*(?:Ğ¼Ğ»Ñ€Ğ´|Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ²?|billion|b)\b',  # ĞœĞ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ñ‹
            r'\b(\d+(?:[\.,]\d+)?)\b'  # ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        ]
        
        for pattern in digit_patterns:
            matches = re.finditer(pattern, text.lower())
            for match in matches:
                number_str = match.group(1) if match.groups() else match.group()
                full_match = match.group()
                
                try:
                    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ğµ ĞºĞ°Ğº Ğ´ĞµÑÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ĞµĞ»Ğ¸
                    number_str = number_str.replace(',', '.')
                    base_number = float(number_str)
                    
                    # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğ¸
                    if any(unit in full_match for unit in ['Ñ‚Ñ‹Ñ', 'Ñ‚Ñ‹ÑÑÑ‡', 'thousand', 'k']):
                        final_number = base_number * 1000
                    elif any(unit in full_match for unit in ['Ğ¼Ğ»Ğ½', 'Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ²', 'Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½', 'million', 'm']):
                        final_number = base_number * 1000000
                    elif any(unit in full_match for unit in ['Ğ¼Ğ»Ñ€Ğ´', 'Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ²', 'Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´', 'billion', 'b']):
                        final_number = base_number * 1000000000
                    else:
                        final_number = base_number
                    
                    extracted_numbers.append({
                        'original': full_match,
                        'value': final_number,
                        'type': 'numeric'
                    })
                except ValueError:
                    continue
        
        # Ğ¡Ğ»Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°
        if language in self.number_words:
            for word, value in self.number_words[language].items():
                if word in text.lower():
                    extracted_numbers.append({
                        'original': word,
                        'value': value,
                        'type': 'word_number'
                    })
        
        return extracted_numbers


class IntentClassifier:
    """ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
    
    def __init__(self):
        self.intent_patterns = {
            'select': [
                r'\b(Ğ¿Ğ¾ĞºĞ°Ğ¶Ğ¸|Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ|Ğ²Ñ‹Ğ²ĞµÑÑ‚Ğ¸|Ğ½Ğ°Ğ¹Ğ´Ğ¸|Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸|Ğ´Ğ°Ğ¹|Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸|list|show|get|select|find|display)\b'
            ],
            'count': [
                r'\b(ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾|Ñ‡Ğ¸ÑĞ»Ğ¾|ĞºĞ¾Ğ»-Ğ²Ğ¾|ÑĞºĞ¾Ğ»ÑŒĞºĞ¾|count|number of)\b'
            ],
            'aggregate': [
                r'\b(ÑÑƒĞ¼Ğ¼Ğ°|Ğ¸Ñ‚Ğ¾Ğ³Ğ¾|Ğ²ÑĞµĞ³Ğ¾|ÑÑ€ĞµĞ´Ğ½ĞµĞµ|ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹|Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼|Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼|sum|total|average|avg|max|min)\b'
            ],
            'filter': [
                r'\b(Ğ³Ğ´Ğµ|Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ĞµĞ¼|Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸|Ğ±Ğ¾Ğ»ÑŒÑˆĞµ|Ğ¼ĞµĞ½ÑŒÑˆĞµ|Ñ€Ğ°Ğ²Ğ½Ğ¾|where|with|having|greater|less|equal)\b'
            ],
            'top': [
                r'\b(Ñ‚Ğ¾Ğ¿|Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ|Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ|top|best|highest|largest)\b'
            ],
            'trend': [
                r'\b(Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°|Ñ‚Ñ€ĞµĞ½Ğ´|Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ|Ñ€Ğ¾ÑÑ‚|ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ|trend|growth|change|over time)\b'
            ],
            'compare': [
                r'\b(ÑÑ€Ğ°Ğ²Ğ½Ğ¸|ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ|Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²|vs|compare|comparison|versus)\b'
            ]
        }
    
    def classify_intent(self, text: str) -> Tuple[Optional[str], float]:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
        text_lower = text.lower()
        scores = {}
        
        for intent, patterns in self.intent_patterns.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, text_lower))
                score += matches
            
            if score > 0:
                scores[intent] = score
        
        if scores:
            intent = max(scores, key=scores.get)
            confidence = scores[intent] / len(text.split())  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°
            return intent, min(confidence, 1.0)
        
        return None, 0.0


class NLNormalizer:
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°"""
    
    def __init__(self):
        self.language_detector = LanguageDetector()
        self.synonym_normalizer = SynonymNormalizer()
        self.datetime_normalizer = DateTimeNormalizer()
        self.number_normalizer = NumberNormalizer()
        self.intent_classifier = IntentClassifier()
    
    def normalize(self, query: str) -> NormalizedQuery:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°"""
        logger.debug(f"Normalizing query: {query}")
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ÑĞ·Ñ‹Ğº
        detected_language = self.language_detector.detect(query)
        logger.debug(f"Detected language: {detected_language}")
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ñ‹
        normalized_text = self.synonym_normalizer.normalize_synonyms(query, detected_language)
        logger.debug(f"After synonym normalization: {normalized_text}")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ´Ğ°Ñ‚Ñ‹
        extracted_dates = self.datetime_normalizer.extract_dates(normalized_text, detected_language)
        normalized_text = self.datetime_normalizer.normalize_dates(normalized_text, detected_language)
        logger.debug(f"After date normalization: {normalized_text}")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ°
        extracted_numbers = self.number_normalizer.extract_numbers(normalized_text, detected_language)
        logger.debug(f"Extracted numbers: {extracted_numbers}")
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ
        intent, confidence = self.intent_classifier.classify_intent(normalized_text)
        logger.debug(f"Detected intent: {intent} (confidence: {confidence})")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°)
        business_terms = self._extract_business_terms(normalized_text, detected_language)
        
        return NormalizedQuery(
            original=query,
            normalized=normalized_text,
            detected_language=detected_language,
            extracted_dates=extracted_dates,
            extracted_numbers=extracted_numbers,
            business_terms=business_terms,
            intent=intent,
            confidence=confidence
        )
    
    def _extract_business_terms(self, text: str, language: Language) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ¸Ğ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°"""
        business_term_patterns = {
            Language.RUSSIAN: [
                'ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹', 'Ğ·Ğ°ĞºĞ°Ğ·Ñ‹', 'Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹', 'Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸', 'Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸',
                'Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°', 'Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ', 'Ğ¼Ğ°Ñ€Ğ¶Ğ°', 'ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹_Ñ‡ĞµĞº',
                'ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'ÑÑƒĞ¼Ğ¼Ğ°', 'ÑÑ€ĞµĞ´Ğ½ĞµĞµ', 'Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼', 'Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼'
            ],
            Language.ENGLISH: [
                'customers', 'orders', 'products', 'sales', 'inventory',
                'revenue', 'profit', 'margin', 'average_order',
                'count', 'sum', 'average', 'maximum', 'minimum'
            ]
        }
        
        terms = []
        if language in business_term_patterns:
            for term in business_term_patterns[language]:
                if term in text.lower():
                    terms.append(term)
        
        return terms


def main():
    """Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Natural Language Normalizer Test')
    parser.add_argument('--query', type=str, required=True, help='Query to normalize')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€
    normalizer = NLNormalizer()
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
    result = normalizer.normalize(args.query)
    
    print(f"ğŸ”¤ Original query: {result.original}")
    print(f"ğŸŒ Detected language: {result.detected_language.value}")
    print(f"âœ¨ Normalized query: {result.normalized}")
    print(f"ğŸ¯ Intent: {result.intent} (confidence: {result.confidence:.3f})")
    
    if result.extracted_dates:
        print(f"ğŸ“… Extracted dates:")
        for date_info in result.extracted_dates:
            print(f"   - {date_info['original']} -> {date_info['sql_expression']}")
    
    if result.extracted_numbers:
        print(f"ğŸ”¢ Extracted numbers:")
        for number_info in result.extracted_numbers:
            print(f"   - {number_info['original']} -> {number_info['value']}")
    
    if result.business_terms:
        print(f"ğŸ’¼ Business terms: {', '.join(result.business_terms)}")


if __name__ == "__main__":
    main()
